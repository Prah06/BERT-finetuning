{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2d2e24",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading\n",
    "\n",
    "### Why Start with this Data?\n",
    "\n",
    "Before building models, we need to understand:\n",
    "- Dataset size (affects model choice)\n",
    "- Class balance (affects evaluation metrics)\n",
    "- Text characteristics (length, complexity)\n",
    "\n",
    "The IMDb dataset is:\n",
    "- **Balanced:** 50/50 positive/negative (no class imbalance bias)\n",
    "- **Medium-sized:** 25k training examples\n",
    "- **Realistic:** Real movie reviews with varied length and complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a574d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prahalad M\\Desktop\\resume_projects\\BERT-finetuning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb dataset from HuggingFace\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba03a34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Examine a sample review\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02133c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews: 12500\n",
      "Negative reviews: 12500\n",
      "\n",
      "✅ Dataset is perfectly balanced (50/50)\n"
     ]
    }
   ],
   "source": [
    "# Verify class balance\n",
    "train_labels = dataset['train']['label']\n",
    "print(f\"Positive reviews: {sum(train_labels)}\")\n",
    "print(f\"Negative reviews: {len(train_labels) - sum(train_labels)}\")\n",
    "print(f\"\\n Dataset is perfectly balanced (50/50)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d2319",
   "metadata": {},
   "source": [
    "## Part 2: Baseline Model - TF-IDF + Logistic Regression\n",
    "\n",
    "### Why Start with a Simple Baseline?\n",
    "\n",
    "**Important principle in ML:** Always establish a simple baseline before using complex models.\n",
    "\n",
    "Benefits:\n",
    "1. **Benchmark:** Provides target to beat\n",
    "2. **Speed:** Fast iteration for testing pipeline\n",
    "3. **Simplicity:** Easy to debug and understand\n",
    "4. **Often sufficient:** Many tasks don't need deep learning\n",
    "\n",
    "### TF-IDF: How It Works\n",
    "\n",
    "**TF-IDF** = Term Frequency × Inverse Document Frequency\n",
    "\n",
    "**Goal:** Identify words that are important to a document\n",
    "\n",
    "**Formula Logic:**\n",
    "- **Term Frequency (TF):** How often does this word appear in THIS review?\n",
    "- **Inverse Document Frequency (IDF):** How rare is this word across ALL reviews?\n",
    "\n",
    "### N-grams: Capturing Phrases\n",
    "\n",
    "Setting `ngram_range=(1,2)` captures:\n",
    "- **Unigrams:** \"good\", \"bad\", \"terrible\"\n",
    "- **Bigrams:** \"not good\", \"very bad\", \"absolutely terrible\"\n",
    "\n",
    "This helps capture negation and emphasis patterns that single words might miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c3d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 25000 reviews\n",
      "Testing on 25000 reviews\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Extract text and labels\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']\n",
    "\n",
    "print(f\"Training on {len(train_texts)} reviews\")\n",
    "print(f\"Testing on {len(test_texts)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e39fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text to TF-IDF features...\n",
      "\n",
      " TF-IDF vectorization complete!\n",
      "Shape: (25000, 5000)\n",
      "Each review → vector of 5000 numbers\n",
      "Parameters: ~10,000 (2 per feature for binary classification)\n"
     ]
    }
   ],
   "source": [
    "# Convert text to TF-IDF features\n",
    "print(\"Converting text to TF-IDF features...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,    # Keep top 5000 most important words\n",
    "    ngram_range=(1, 2)    # Use both single words and two-word phrases\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "print(f\"\\n TF-IDF vectorization complete!\")\n",
    "print(f\"Shape: {X_train.shape}\")\n",
    "print(f\"Each review → vector of {X_train.shape[1]} numbers\")\n",
    "print(f\"Parameters: ~{X_train.shape[1] * 2:,} (2 per feature for binary classification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9bb91",
   "metadata": {},
   "source": [
    "### Logistic Regression for Classification\n",
    "\n",
    "**Why Logistic Regression?**\n",
    "- Fast training on high-dimensional data\n",
    "- Probabilistic outputs (confidence scores)\n",
    "- Works well with TF-IDF features\n",
    "- Interpretable (can examine feature weights)\n",
    "\n",
    "**What it learns:**\n",
    "- Positive weights for words like \"excellent\", \"amazing\", \"loved\"\n",
    "- Negative weights for words like \"terrible\", \"boring\", \"waste\"\n",
    "- Combines these weights to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3604a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "============================================================\n",
      "BASELINE MODEL PERFORMANCE\n",
      "============================================================\n",
      "Test Accuracy: 0.8884 (88.84%)\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.88      0.89     12500\n",
      "    Positive       0.88      0.89      0.89     12500\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression model...\")\n",
    "baseline_model = LogisticRegression(max_iter=1000)\n",
    "baseline_model.fit(X_train, train_labels)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "predictions = baseline_model.predict(X_test)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BASELINE MODEL PERFORMANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=['Negative', 'Positive']))\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c6f67",
   "metadata": {},
   "source": [
    "### Baseline Results Analysis\n",
    "\n",
    "**Metrics explanation:**\n",
    "- **Accuracy:** Overall correctness (~88.84%)\n",
    "- **Precision:** When model predicts positive, how often is it right? (~89%)\n",
    "- **Recall:** Of all actual positive reviews, how many did we find? (~88%)\n",
    "- **F1-Score:** Harmonic mean of precision and recall (~0.89)\n",
    "\n",
    "**Key observations:**\n",
    "- Balanced performance across both classes \n",
    "- Simple model achieving ~89% accuracy\n",
    "- This is our benchmark - BERT needs to beat this to be worthwhile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad72e57",
   "metadata": {},
   "source": [
    "## Part 3: BERT - Transformer Architecture\n",
    "\n",
    "### What is BERT?\n",
    "\n",
    "**BERT** = Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "**Key innovation:** Processes text bidirectionally (looks both left AND right)\n",
    "\n",
    "### Traditional Models vs BERT\n",
    "\n",
    "**Traditional (RNN/LSTM):**\n",
    "```\n",
    "Reads left-to-right sequentially:\n",
    "\"This\" → \"movie\" → \"wasn't\" → \"good\"\n",
    "\n",
    "Problem: May forget \"wasn't\" by the time it reaches \"good\"\n",
    "```\n",
    "\n",
    "**BERT (Transformer):**\n",
    "```\n",
    "Processes all words simultaneously:\n",
    "\"This\" ↔ \"movie\" ↔ \"wasn't\" ↔ \"good\"\n",
    "\n",
    "Advantage: \"good\" can attend to \"wasn't\" → understands negation\n",
    "```\n",
    "\n",
    "### The Attention Mechanism\n",
    "\n",
    "**Core concept:** Each word \"attends\" to (looks at) every other word\n",
    "\n",
    "**Example: \"This movie wasn't good\"**\n",
    "\n",
    "When processing \"good\":\n",
    "```\n",
    "Attention weights (learned automatically):\n",
    "\"This\"   → 5%  (low attention)\n",
    "\"movie\"  → 15% (medium attention)\n",
    "\"wasn't\" → 65% (HIGH attention - this is key!)\n",
    "\"good\"   → 15% (self-attention)\n",
    "```\n",
    "\n",
    "Result: \"good\" incorporates meaning from \"wasn't\" → understands the phrase is negative!\n",
    "\n",
    "### BERT Architecture\n",
    "\n",
    "**Layers:**\n",
    "- **12 transformer layers** (BERT-base)\n",
    "- Each layer refines understanding progressively\n",
    "\n",
    "**What each layer learns (conceptually):**\n",
    "```\n",
    "Layer 1-2:   Basic syntax and grammar\n",
    "Layer 3-5:   Phrase relationships\n",
    "Layer 6-8:   Semantic meaning and context\n",
    "Layer 9-12:  Complex sentence-level understanding\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- Total: 109,483,778 parameters\n",
    "- Embeddings: ~23M (vocabulary × dimensions)\n",
    "- Transformer layers: ~84M (12 layers × ~7M each)\n",
    "- Classification head: ~2M\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "**Two-stage process:**\n",
    "\n",
    "**Stage 1: Pre-training (Done by Google)**\n",
    "- Trained on massive text corpus (Wikipedia + BookCorpus)\n",
    "- Learned general language understanding\n",
    "- Result: Model understands English grammar, syntax, semantics\n",
    "\n",
    "**Stage 2: Fine-tuning (What we're doing)**\n",
    "- Start with pre-trained BERT\n",
    "- Train on our specific task (sentiment classification)\n",
    "- Only need 25k examples (vs billions for pre-training)\n",
    "- Result: BERT adapted for sentiment analysis\n",
    "\n",
    "**Analogy:** \n",
    "- Pre-training = Learning to read and understand English\n",
    "- Fine-tuning = Learning to identify sentiment in reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766018c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model...\n",
      "\n",
      " Model and tokenizer loaded!\n",
      "Model has 109,483,778 parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Loading BERT model...\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2  # Binary classification: positive/negative\n",
    ")\n",
    "\n",
    "print(\"\\n Model and tokenizer loaded!\")\n",
    "print(f\"Model has {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c705dd",
   "metadata": {},
   "source": [
    "### BERT Tokenization\n",
    "\n",
    "**Process:** Convert text → numbers BERT understands\n",
    "\n",
    "**Steps:**\n",
    "1. **Lowercase:** \"This Movie\" → \"this movie\"\n",
    "2. **Add special tokens:** \"[CLS] this movie was amazing [SEP]\"\n",
    "   - `[CLS]`: Classification token (sentence-level representation)\n",
    "   - `[SEP]`: Separator (marks end of sentence)\n",
    "3. **Subword tokenization:** \"unbelievable\" → [\"un\", \"##believable\"]\n",
    "   - Handles unknown words by breaking into pieces\n",
    "4. **Convert to IDs:** Map each token to number from vocabulary (30,000 words)\n",
    "5. **Pad/Truncate:** Make all sequences same length (128 tokens)\n",
    "6. **Attention mask:** 1 for real tokens, 0 for padding\n",
    "\n",
    "**Why max_length=128?**\n",
    "- Captures most review content (first ~100 words)\n",
    "- Balances context vs computation speed\n",
    "- Longer = more context but slower training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf97ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n",
      "Tokenizing test data...\n",
      "\n",
      " Tokenization complete!\n",
      "Training shape: torch.Size([25000, 128])\n",
      "Test shape: torch.Size([25000, 128])\n",
      "\n",
      "Each review → [128] token IDs + [128] attention mask\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing training data...\")\n",
    "train_encodings = tokenizer(\n",
    "    list(train_texts),\n",
    "    truncation=True,      \n",
    "    padding=True,         \n",
    "    max_length=128,       \n",
    "    return_tensors='pt'  \n",
    ")\n",
    "\n",
    "print(\"Tokenizing test data...\")\n",
    "test_encodings = tokenizer(\n",
    "    list(test_texts),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(\"\\n Tokenization complete!\")\n",
    "print(f\"Training shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"Test shape: {test_encodings['input_ids'].shape}\")\n",
    "print(f\"\\nEach review → [128] token IDs + [128] attention mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242d975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and DataLoaders created!\n",
      "Training batches per epoch: 1563\n",
      "Test batches: 1563\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for sentiment analysis\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Datasets and DataLoaders created!\")\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad7411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      " Training setup complete!\n",
      "Total training steps: 4689\n",
      "Steps per epoch: 1563\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=2e-5,              # Learning rate\n",
    "    weight_decay=0.01     # Regularization\n",
    ")\n",
    "\n",
    "# Setup scheduler\n",
    "epochs = 3\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"\\n Training setup complete!\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Steps per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f59b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct_predictions/total_predictions:.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training and evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f9ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING BERT FINE-TUNING\n",
      "============================================================\n",
      "Device: cuda\n",
      "Epochs: 3\n",
      "Batch size: 16\n",
      "Learning rate: 2e-5\n",
      "Total parameters: 109,483,778\n",
      "============================================================\n",
      "\n",
      " EPOCH 1/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 154/1563 [01:23<12:24,  1.89it/s, loss=0.3692, acc=0.8312]"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STARTING BERT FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Batch size: 16\")\n",
    "print(f\"Learning rate: 2e-5\")\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n EPOCH {epoch + 1}/{epochs}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Calculate train-test gap\n",
    "    gap = train_acc - test_acc\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n Results:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"Train-Test Gap: {gap*100:.2f}%\", end=\" \")\n",
    "    \n",
    "    if gap > 0.05:\n",
    "        print(\"Overfitting detected!\")\n",
    "    elif gap > 0:\n",
    "        print(\"Starting to overfit\")\n",
    "    else:\n",
    "        print(\"Good generalization\")\n",
    "    \n",
    "    if test_acc > best_accuracy:\n",
    "        best_accuracy = test_acc\n",
    "        print(f\"New best accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"Best Test Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"Improvement over baseline: {(best_accuracy - 0.8884)*100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad599e",
   "metadata": {},
   "source": [
    "## Part 5: Results Analysis & Conclusions\n",
    "\n",
    "### Comparing BERT vs Baseline\n",
    "\n",
    "**Final Results:**\n",
    "- **Baseline (TF-IDF):** 88.84% accuracy, no overfitting\n",
    "- **BERT (3 epochs):** ~88.97% accuracy, severe overfitting\n",
    "\n",
    "### Overfitting Analysis\n",
    "\n",
    "**Typical pattern observed:**\n",
    "```\n",
    "Epoch 1: Train 85%, Test 89% (Test better - healthy!)\n",
    "Epoch 2: Train 93%, Test 89% (Gap opening)\n",
    "Epoch 3: Train 97%, Test 89% (Severe overfitting)\n",
    "```\n",
    "\n",
    "**What happened:**\n",
    "- Training accuracy jumped 12% (85% → 97%)\n",
    "- Test accuracy stayed flat (~89%)\n",
    "- Model memorized training data instead of learning patterns\n",
    "\n",
    "**Root cause:**\n",
    "- 109M parameters ÷ 25k examples = 4,379 params/example\n",
    "- Model has enough capacity to memorize every training example\n",
    "- Insufficient data to force generalizable learning\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "**1. Dataset Size Matters**\n",
    "- 25k examples insufficient for 109M parameter model\n",
    "- Transformers typically need 100k+ examples\n",
    "- Rule: Match model complexity to data availability\n",
    "\n",
    "**2. Simple Baselines Can Win**\n",
    "- TF-IDF achieved 88.84% with no overfitting\n",
    "- BERT achieved 88.97% with severe overfitting\n",
    "- 0.13% gain not worth the complexity\n",
    "\n",
    "**3. Overfitting Detection**\n",
    "- Train-test gap is key metric\n",
    "- Monitor from Epoch 1\n",
    "- Early stopping could have helped\n",
    "\n",
    "**4. Production Considerations**\n",
    "- TF-IDF: <1 min training, simple deployment\n",
    "- BERT: 39 min training, complex infrastructure\n",
    "- For 0.13% gain, simplicity wins\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "**Always establish simple baselines first**  \n",
    "**Model complexity should match dataset size**  \n",
    "**Monitor train-test gap for overfitting**  \n",
    "**Consider cost-benefit in model selection**  \n",
    "**More parameters ≠ better performance**  \n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Use TF-IDF + Logistic Regression when:**\n",
    "- Dataset < 100k examples\n",
    "- Binary/simple classification\n",
    "- Speed and simplicity valued\n",
    "- Interpretability required\n",
    "\n",
    "**Use BERT/Transformers when:**\n",
    "- Dataset > 100k examples\n",
    "- Complex NLP tasks (NER, QA, summarization)\n",
    "- Context-dependent understanding critical\n",
    "- You have compute budget and time\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "To improve BERT performance:\n",
    "1. **More data:** Increase to 100k+ examples\n",
    "2. **Early stopping:** Stop at Epoch 1\n",
    "3. **Smaller model:** Try DistilBERT (66M params)\n",
    "4. **Regularization:** Increase dropout rates\n",
    "5. **Data augmentation:** Paraphrasing, back-translation\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion:** This project demonstrates that state-of-the-art models aren't always the best choice. Model selection should be driven by dataset characteristics, task requirements, and production constraints - not just pursuing the latest architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
