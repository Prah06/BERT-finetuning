{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2171fef",
   "metadata": {},
   "source": [
    "# BERT Fine-tuning for Sentiment Analysis\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained BERT model for sentiment analysis using the IMDB movie reviews dataset. We'll compare the performance of a traditional machine learning baseline (TF-IDF + Logistic Regression) with a fine-tuned BERT model.\n",
    "\n",
    "## üéØ **Project Goals:**\n",
    "- Load and explore the IMDB dataset\n",
    "- Establish a baseline using traditional ML methods\n",
    "- Fine-tune BERT for binary sentiment classification\n",
    "- Compare performance between baseline and BERT models\n",
    "\n",
    "## üìä **Expected Outcomes:**\n",
    "- **Baseline Model**: ~88-89% accuracy\n",
    "- **Fine-tuned BERT**: ~92-94% accuracy\n",
    "- **Learning**: Understanding transformer-based models vs traditional approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb5587",
   "metadata": {},
   "source": [
    "## üìö Step 1: Dataset Loading and Exploration\n",
    "\n",
    "First, we'll load the famous IMDB movie reviews dataset which contains 50,000 reviews (25k for training, 25k for testing) labeled as positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prahalad M\\Desktop\\resume_projects\\BERT-finetuning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset using Hugging Face datasets library\n",
    "# This dataset contains 25k training and 25k test movie reviews\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download and load the dataset (may take a few minutes on first run)\n",
    "dataset = load_dataset(\"imdb\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Explore the structure of our dataset\n",
    "# This shows us the training and test splits with their sizes\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Look at a sample review to understand the data structure\n",
    "# Each sample contains 'text' (review content) and 'label' (0=negative, 1=positive)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be5f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postive reviews: 12500\n",
      "negative reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "# Check the class balance in our training data\n",
    "# Labels: 0 = negative sentiment, 1 = positive sentiment\n",
    "train_labels = dataset['train']['label']\n",
    "print(f\"Positive reviews: {sum(train_labels)}\")\n",
    "print(f\"Negative reviews: {len(train_labels) - sum(train_labels)}\")\n",
    "print(f\"Dataset is {'balanced' if sum(train_labels) == len(train_labels) - sum(train_labels) else 'imbalanced'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59361af3",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: Baseline Model (Traditional ML Approach)\n",
    "\n",
    "Before jumping into BERT, let's establish a baseline using traditional machine learning:\n",
    "- **TF-IDF**: Convert text to numerical features\n",
    "- **Logistic Regression**: Simple but effective classifier\n",
    "\n",
    "This helps us understand how much improvement BERT provides over classical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 25000 reviews\n",
      "Testing on 25000 reviews\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for our baseline machine learning model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Prepare our text data for traditional ML processing\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']  \n",
    "test_labels = dataset['test']['label']\n",
    "test_texts = dataset['test']['text']\n",
    "\n",
    "# Show dataset size information\n",
    "print(f\"Training on {len(train_texts):,} reviews\")\n",
    "print(f\"Testing on {len(test_texts):,} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text to TF-IDF features\n",
      "TF-IDF shape: (25000, 5000)\n",
      "Each review is represented by a vector of length 5000 numbers\n"
     ]
    }
   ],
   "source": [
    "# Convert text to numerical features using TF-IDF\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) measures word importance\n",
    "# - Higher values = words that appear frequently in this doc but rarely in others\n",
    "# - ngram_range=(1,2) = consider both single words and word pairs\n",
    "print(\"Converting text to TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,     # Keep only the 5000 most important features\n",
    "    ngram_range=(1,2)      # Use single words and word pairs (bigrams)\n",
    ")\n",
    "\n",
    "# Fit vectorizer on training data and transform both train and test sets\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "print(f\"‚úÖ TF-IDF transformation complete!\")\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Each review is now represented by {X_train.shape[1]:,} numerical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758f0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model\n",
      "Making predictions on test set\n",
      "\n",
      "==================================================\n",
      "BASELINE MODEL PERFORMANCE\n",
      "==================================================\n",
      "Accuracy: 0.8884 (88.84%)\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.88      0.89     12500\n",
      "    Positive       0.88      0.89      0.89     12500\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train our baseline logistic regression model\n",
    "print(\"üéØ Training Logistic Regression baseline model...\")\n",
    "baseline_model = LogisticRegression(max_iter=1000)\n",
    "baseline_model.fit(X_train, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"üìä Making predictions on test set...\")\n",
    "predictions = baseline_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üèÜ BASELINE MODEL PERFORMANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=['Negative', 'Positive']))\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"üí° This baseline gives us a target to beat with BERT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae815774",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: BERT Model Setup\n",
    "\n",
    "Now let's implement the star of our show - BERT! \n",
    "\n",
    "**What is BERT?**\n",
    "- **B**idirectional **E**ncoder **R**epresentations from **T**ransformers\n",
    "- Pre-trained on massive text corpora (Wikipedia + BookCorpus)\n",
    "- Understands context from both left AND right sides of words\n",
    "- State-of-the-art performance on many NLP tasks\n",
    "\n",
    "**Why BERT is powerful:**\n",
    "- Captures complex language patterns and relationships\n",
    "- Pre-trained knowledge can be fine-tuned for specific tasks\n",
    "- Bidirectional context understanding (unlike traditional left-to-right models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Loading BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and tokenizer loaded!\n",
      "Model has 109,483,778 parameters\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT components from Hugging Face\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "print(\"üîÑ Loading BERT tokenizer...\")\n",
    "# The tokenizer converts text into tokens that BERT can understand\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"üîÑ Loading pre-trained BERT model...\")\n",
    "# Load BERT with a classification head for binary sentiment analysis\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',    # 12-layer, 768-hidden, 12-heads, 110M parameters\n",
    "    num_labels=2            # Binary classification: positive/negative\n",
    ")\n",
    "\n",
    "print(\"‚úÖ BERT model and tokenizer loaded successfully!\")\n",
    "print(f\"üìä Model size: {model.num_parameters():,} parameters\")\n",
    "\n",
    "# üîç **What's happening under the hood:**\n",
    "#\n",
    "# 1Ô∏è‚É£ **bert-base-uncased Model Architecture:**\n",
    "#    - 12 transformer layers (compared to 24 in bert-large)\n",
    "#    - 768 hidden dimensions\n",
    "#    - 12 attention heads per layer\n",
    "#    - ~110 million trainable parameters\n",
    "#    - \"uncased\" = not case-sensitive (converts \"Hello\" ‚Üí \"hello\")\n",
    "#\n",
    "# 2Ô∏è‚É£ **BertTokenizer Functions:**\n",
    "#    - Vocabulary: 30,522 unique tokens\n",
    "#    - Subword tokenization: \"unbelievable\" ‚Üí [\"un\", \"##believe\", \"##able\"]\n",
    "#    - Special tokens: [CLS] (start), [SEP] (separator), [PAD] (padding)\n",
    "#    - Handles out-of-vocabulary words gracefully\n",
    "#\n",
    "# 3Ô∏è‚É£ **BertForSequenceClassification:**\n",
    "#    - Pre-trained BERT encoder + classification head\n",
    "#    - Classification head: dropout + linear layer (768 ‚Üí 2 outputs)\n",
    "#    - Only the classification head is randomly initialized\n",
    "#    - BERT weights start from pre-trained values\n",
    "#\n",
    "# üíæ **First-time setup:**\n",
    "# - Downloads ~440MB of model weights\n",
    "# - Caches locally for future use\n",
    "# - Takes 1-2 minutes depending on internet speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375af0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n",
      "Tokenizing test data...\n",
      "‚úÖ Tokenization complete!\n",
      "Training shape: torch.Size([25000, 128])\n",
      "Test shape: torch.Size([25000, 128])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize our text data for BERT processing\n",
    "# BERT requires specific input format with special tokens and padding\n",
    "\n",
    "print(\"üîÑ Tokenizing training data...\")\n",
    "train_encodings = tokenizer(\n",
    "    list(train_texts),\n",
    "    truncation=True,      # Cut off reviews longer than max_length\n",
    "    padding=True,         # Pad shorter reviews to uniform length\n",
    "    max_length=128,       # Maximum sequence length (balance between speed and content)\n",
    "    return_tensors='pt'   # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(\"üîÑ Tokenizing test data...\")\n",
    "test_encodings = tokenizer(\n",
    "    list(test_texts),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization complete!\")\n",
    "print(f\"üìä Training data shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"üìä Test data shape: {test_encodings['input_ids'].shape}\")\n",
    "print(f\"\\nüîç What each dimension means:\")\n",
    "print(f\"   ‚Ä¢ First dimension ({train_encodings['input_ids'].shape[0]:,}): Number of reviews\")\n",
    "print(f\"   ‚Ä¢ Second dimension ({train_encodings['input_ids'].shape[1]}): Sequence length (tokens per review)\")\n",
    "\n",
    "# üí° **Understanding the tokenization process:**\n",
    "#\n",
    "# Input: \"This movie is great!\"\n",
    "# ‚Üì\n",
    "# Tokens: [\"[CLS]\", \"this\", \"movie\", \"is\", \"great\", \"!\", \"[SEP]\", \"[PAD]\", \"[PAD]\", ...]\n",
    "# ‚Üì\n",
    "# Token IDs: [101, 2023, 3185, 2003, 2307, 999, 102, 0, 0, ...]\n",
    "# ‚Üì \n",
    "# Attention Mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, ...] (1=real token, 0=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350eb1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets created!\n",
      "Training dataset size: 25000\n",
      "Test dataset size: 25000\n",
      "\n",
      "Sample data structure:\n",
      "  input_ids shape: torch.Size([128])\n",
      "  attention_mask shape: torch.Size([128])\n",
      "  label: 0\n"
     ]
    }
   ],
   "source": [
    "# Create custom PyTorch Dataset classes for efficient data loading\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class that packages our tokenized text with labels\n",
    "    for efficient batch processing during training\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset\"\"\"\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset\n",
    "        Returns: dictionary with input_ids, attention_mask, and labels\n",
    "        \"\"\"\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Create dataset objects for training and testing\n",
    "print(\"üîÑ Creating PyTorch datasets...\")\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "print(\"‚úÖ Dataset objects created successfully!\")\n",
    "print(f\"üìä Training dataset size: {len(train_dataset):,} reviews\")\n",
    "print(f\"üìä Test dataset size: {len(test_dataset):,} reviews\")\n",
    "\n",
    "# Inspect a sample to understand the data structure\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nüîç Sample data structure:\")\n",
    "print(f\"   ‚Ä¢ input_ids shape: {sample['input_ids'].shape} (tokenized text)\")\n",
    "print(f\"   ‚Ä¢ attention_mask shape: {sample['attention_mask'].shape} (padding mask)\")\n",
    "print(f\"   ‚Ä¢ label: {sample['labels']} ({'positive' if sample['labels'] == 1 else 'negative'} sentiment)\")\n",
    "\n",
    "# üí° **Why we need custom Dataset classes:**\n",
    "#\n",
    "# üéØ **Efficient batch processing**: PyTorch can automatically batch our data\n",
    "# üéØ **Memory management**: Load data on-demand rather than keeping everything in memory\n",
    "# üéØ **Standardized interface**: Works seamlessly with PyTorch DataLoaders\n",
    "# üéØ **Flexibility**: Easy to add data augmentation or preprocessing later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305ea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created!\n",
      "Training batches per epoch: 1563\n",
      "Test batches: 1563\n",
      "Batch size: 16 reviews\n",
      "\n",
      "Each epoch will process 1563 batches\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for efficient batch processing during training\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up data loaders with appropriate batch sizes\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16,      # Process 16 reviews at once (balance memory vs speed)\n",
    "    shuffle=True        # Shuffle training data for better learning\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=16,      # Same batch size for consistency\n",
    "    shuffle=False       # No need to shuffle test data\n",
    ")\n",
    "\n",
    "print('‚úÖ DataLoaders created successfully!')\n",
    "print(f\"üìä Training batches per epoch: {len(train_loader):,}\")\n",
    "print(f\"üìä Test batches: {len(test_loader):,}\")\n",
    "print(f\"üìä Batch size: 16 reviews per batch\")\n",
    "print(f\"\\nüîÑ Training process:\")\n",
    "print(f\"   ‚Ä¢ Each epoch processes {len(train_loader):,} batches\")\n",
    "print(f\"   ‚Ä¢ Total training samples per epoch: {len(train_loader) * 16:,}\")\n",
    "print(f\"   ‚Ä¢ Estimated time per epoch: ~8-12 minutes (depending on GPU)\")\n",
    "\n",
    "# üí° **DataLoader benefits:**\n",
    "#\n",
    "# üöÄ **Batch processing**: Train on multiple samples simultaneously\n",
    "# üß† **Memory efficiency**: Load batches on-demand, not entire dataset\n",
    "# üîÑ **Automatic shuffling**: Prevents model from memorizing data order\n",
    "# ‚ö° **Parallel loading**: Can use multiple CPU cores for data loading\n",
    "# üéØ **Consistent interface**: Standard PyTorch training loop compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ PROJECT PROGRESS TRACKER üöÄ\n",
    "# \n",
    "# [‚úÖ Setup] ‚Üí [‚úÖ Data Loading] ‚Üí [‚úÖ Baseline Model] ‚Üí [‚úÖ BERT Preparation] ‚Üí [üî• TRAINING] ‚Üê YOU ARE HERE ‚Üí [Evaluation] ‚Üí [Results]\n",
    "#\n",
    "# üéØ Ready to fine-tune BERT! All preprocessing complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c9357",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 4: Training Configuration\n",
    "\n",
    "Before we start training, we need to set up our training environment and hyperparameters. This includes:\n",
    "- **Device selection** (GPU vs CPU)\n",
    "- **Optimizer configuration** (how the model learns)\n",
    "- **Learning rate scheduling** (adjusts learning speed during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4560d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model moved to device!\n"
     ]
    }
   ],
   "source": [
    "# Configure the training device (GPU vs CPU)\n",
    "import torch \n",
    "\n",
    "# Automatically detect and use GPU if available, otherwise fall back to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No GPU detected - training will be slower on CPU\")\n",
    "\n",
    "# Move our model to the selected device (GPU/CPU)\n",
    "model = model.to(device)\n",
    "print(f\"‚úÖ Model moved to {device}!\")\n",
    "\n",
    "# üöÄ **GPU vs CPU Performance:**\n",
    "#\n",
    "# üî• **With GPU (CUDA)**:\n",
    "#    ‚Ä¢ Training time: ~25-30 minutes for 3 epochs\n",
    "#    ‚Ä¢ Memory usage: ~6-8GB GPU memory\n",
    "#    ‚Ä¢ Batch size: 16 (or higher with more memory)\n",
    "#\n",
    "# ‚è≥ **With CPU only**:\n",
    "#    ‚Ä¢ Training time: ~3-4 hours for 3 epochs\n",
    "#    ‚Ä¢ Memory usage: ~8-12GB RAM\n",
    "#    ‚Ä¢ Batch size: Limited by available RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa06a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimizer configured!\n",
      "Learning rate: 2e-5\n",
      "Total training steps: 4689\n",
      "Epochs: 3\n",
      "Steps per epoch: 1563\n"
     ]
    }
   ],
   "source": [
    "# Configure optimizer and learning rate scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Set up AdamW optimizer with weight decay (prevents overfitting)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=2e-5,             # Learning rate: small value for fine-tuning\n",
    "    weight_decay=0.01    # L2 regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "epochs = 3  # Number of complete passes through the dataset\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Learning rate scheduler (starts low, warms up, then decreases)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,           # No warmup (could use 100-500 for very large datasets)\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration complete!\")\n",
    "print(f\"üìä Training hyperparameters:\")\n",
    "print(f\"   ‚Ä¢ Learning rate: 2e-5 (optimized for BERT fine-tuning)\")\n",
    "print(f\"   ‚Ä¢ Weight decay: 0.01 (regularization)\")\n",
    "print(f\"   ‚Ä¢ Epochs: {epochs}\")\n",
    "print(f\"   ‚Ä¢ Total training steps: {total_steps:,}\")\n",
    "print(f\"   ‚Ä¢ Steps per epoch: {len(train_loader):,}\")\n",
    "\n",
    "# üß† **Why these hyperparameters?**\n",
    "#\n",
    "# üéØ **Learning Rate (2e-5)**:\n",
    "#    ‚Ä¢ BERT is pre-trained, so we need small updates\n",
    "#    ‚Ä¢ Too high ‚Üí catastrophic forgetting of pre-trained knowledge\n",
    "#    ‚Ä¢ Too low ‚Üí very slow learning or poor convergence\n",
    "#    ‚Ä¢ 2e-5 is the sweet spot found by research\n",
    "#\n",
    "# ‚öñÔ∏è **Weight Decay (0.01)**:\n",
    "#    ‚Ä¢ Prevents overfitting by penalizing large weights\n",
    "#    ‚Ä¢ Standard value for transformer fine-tuning\n",
    "#\n",
    "# üîÑ **Linear Decay Schedule**:\n",
    "#    ‚Ä¢ Learning rate decreases linearly over time\n",
    "#    ‚Ä¢ Helps model converge to optimal solution\n",
    "#    ‚Ä¢ Alternative: cosine decay or constant rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee595fd",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 5: Training Functions\n",
    "\n",
    "Now we'll define our training and evaluation functions. These functions handle:\n",
    "- **Forward pass**: Data ‚Üí Model ‚Üí Predictions\n",
    "- **Backward pass**: Calculate gradients and update weights\n",
    "- **Evaluation**: Track performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a98f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training and evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Define training and evaluation functions\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    Train the model for one complete epoch\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss (float): Average training loss\n",
    "        accuracy (float): Training accuracy\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode (enables dropout, etc.)\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Progress bar for visual feedback\n",
    "    progress_bar = tqdm(dataloader, desc='üî• Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch data to device (GPU/CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Reset gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: input ‚Üí model ‚Üí predictions\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss      # Cross-entropy loss\n",
    "        logits = outputs.logits  # Raw prediction scores\n",
    "        \n",
    "        # Backward pass: calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "        \n",
    "        # Track performance metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1)  # Convert scores to predictions\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        # Update progress bar with current metrics\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct_predictions/total_predictions:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on test/validation data\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss (float): Average evaluation loss\n",
    "        accuracy (float): Evaluation accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Disable gradient computation for faster inference\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='üìä Evaluating'):\n",
    "            # Move batch data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass only (no backpropagation)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"‚úÖ Training and evaluation functions defined!\")\n",
    "print(\"\\nüîç Function overview:\")\n",
    "print(\"   ‚Ä¢ train_epoch(): Trains model for one epoch, updates weights\")\n",
    "print(\"   ‚Ä¢ evaluate(): Tests model performance without updating weights\")\n",
    "print(\"   ‚Ä¢ Both functions track loss and accuracy metrics\")\n",
    "print(\"   ‚Ä¢ Progress bars show real-time training/evaluation progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153e9cbf",
   "metadata": {},
   "source": [
    "## üéØ Step 6: BERT Fine-tuning Training Loop\n",
    "\n",
    "This is where the magic happens! We'll train our BERT model for 3 epochs and watch it learn to understand movie review sentiments.\n",
    "\n",
    "**What to expect:**\n",
    "- **Epoch 1**: ~89-91% accuracy (rapid initial learning)\n",
    "- **Epoch 2**: ~91-93% accuracy (fine-tuning improvements)\n",
    "- **Epoch 3**: ~92-94% accuracy (convergence)\n",
    "\n",
    "**Training time estimates:**\n",
    "- With GPU: ~25-30 minutes total\n",
    "- With CPU: ~3-4 hours total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ STARTING BERT FINE-TUNING\n",
      "============================================================\n",
      "Device: cuda\n",
      "Epochs: 3\n",
      "Batch size: 16\n",
      "Learning rate: 2e-5\n",
      "Total parameters: 109,483,778\n",
      "============================================================\n",
      "\n",
      "üìç EPOCH 1/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [13:14<00:00,  1.97it/s, loss=0.3177, acc=0.9024]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [04:01<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Results:\n",
      "  Train Loss: 0.2380 | Train Acc: 0.9024 (90.24%)\n",
      "  Test Loss:  0.2777 | Test Acc:  0.8892 (88.92%)\n",
      "  üèÜ New best accuracy: 0.8892\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìç EPOCH 2/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [23:04<00:00,  1.13it/s, loss=0.1062, acc=0.9592]    \n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [04:10<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Results:\n",
      "  Train Loss: 0.1153 | Train Acc: 0.9592 (95.92%)\n",
      "  Test Loss:  0.3248 | Test Acc:  0.8873 (88.73%)\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìç EPOCH 3/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [5:09:49<00:00, 11.89s/it, loss=0.0032, acc=0.9887]     \n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1563/1563 [04:14<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Results:\n",
      "  Train Loss: 0.0409 | Train Acc: 0.9887 (98.87%)\n",
      "  Test Loss:  0.4162 | Test Acc:  0.8897 (88.97%)\n",
      "  üèÜ New best accuracy: 0.8897\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "‚úÖ TRAINING COMPLETE!\n",
      "üèÜ Best Test Accuracy: 0.8897 (88.97%)\n",
      "üìà Improvement over baseline: 0.13%\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# üöÄ MAIN TRAINING LOOP - FINE-TUNE BERT FOR SENTIMENT ANALYSIS! üöÄ\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ BERT FINE-TUNING TRAINING STARTED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "print(f\"üìä Epochs: {epochs}\")\n",
    "print(f\"üî¢ Batch size: 16 reviews per batch\")\n",
    "print(f\"üìà Learning rate: 2e-5\")\n",
    "print(f\"‚öôÔ∏è  Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"üéØ Target: Beat baseline accuracy of ~88.84%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Track the best performance\n",
    "best_accuracy = 0\n",
    "training_history = []\n",
    "\n",
    "# Training loop: repeat for specified number of epochs\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nüî• EPOCH {epoch + 1}/{epochs}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Train for one epoch\n",
    "    print(\"üèãÔ∏è  Training phase...\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"üìä Evaluation phase...\")\n",
    "    test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Store results\n",
    "    training_history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "    \n",
    "    # Display results for this epoch\n",
    "    print(f\"\\nüìã EPOCH {epoch + 1} RESULTS:\")\n",
    "    print(f\"   üèãÔ∏è  Training   ‚Üí Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"   üìä Test       ‚Üí Loss: {test_loss:.4f} | Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    if test_acc > best_accuracy:\n",
    "        best_accuracy = test_acc\n",
    "        improvement = \"üÜï NEW BEST!\" if epoch > 0 else \"üéØ BASELINE SET\"\n",
    "        print(f\"   üèÜ {improvement} Best accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"   üìà Best so far: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "\n",
    "# Final summary\n",
    "baseline_accuracy = 0.8884  # From our logistic regression baseline\n",
    "improvement = (best_accuracy - baseline_accuracy) * 100\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"üéâ TRAINING COMPLETED! üéâ\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üèÜ Best BERT accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"ü§ñ Baseline accuracy: {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
    "print(f\"üìà Improvement: {improvement:+.2f} percentage points\")\n",
    "print(f\"üöÄ Relative improvement: {(improvement/baseline_accuracy)*100:+.1f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Performance interpretation\n",
    "if best_accuracy > 0.92:\n",
    "    print(\"üî• EXCELLENT! Your BERT model achieved outstanding performance!\")\n",
    "elif best_accuracy > 0.90:\n",
    "    print(\"‚úÖ GREAT! Your BERT model shows significant improvement over baseline!\")\n",
    "elif best_accuracy > baseline_accuracy:\n",
    "    print(\"üëç GOOD! BERT outperformed the baseline, which is expected!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Hmm, something might be off. BERT should typically beat the baseline.\")\n",
    "\n",
    "print(f\"\\nüí° Key takeaways:\")\n",
    "print(f\"   ‚Ä¢ BERT's bidirectional context understanding ‚Üí Better sentiment analysis\")\n",
    "print(f\"   ‚Ä¢ Transfer learning from pre-trained knowledge ‚Üí Faster convergence\")  \n",
    "print(f\"   ‚Ä¢ Fine-tuning approach ‚Üí Domain-specific adaptation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632011bf",
   "metadata": {},
   "source": [
    "## üéä Congratulations! \n",
    "\n",
    "You've successfully fine-tuned BERT for sentiment analysis! \n",
    "\n",
    "### üîç **What You've Learned:**\n",
    "\n",
    "1. **Traditional ML vs Deep Learning**: Saw the performance difference between TF-IDF + Logistic Regression vs BERT\n",
    "2. **Transfer Learning**: Leveraged pre-trained BERT knowledge for your specific task\n",
    "3. **Fine-tuning Process**: Understood how to adapt pre-trained models to new domains\n",
    "4. **PyTorch Training Loop**: Implemented a complete training pipeline with proper evaluation\n",
    "\n",
    "### üöÄ **Next Steps:**\n",
    "\n",
    "- **Try different models**: experiment with RoBERTa, DistilBERT, or domain-specific models\n",
    "- **Hyperparameter tuning**: adjust learning rates, batch sizes, or training epochs\n",
    "- **Real-world deployment**: integrate your model into a web application or API\n",
    "- **Advanced techniques**: implement techniques like gradient accumulation or mixed precision training\n",
    "\n",
    "### üìö **Key Concepts Mastered:**\n",
    "\n",
    "- **Tokenization**: Converting text to model-readable format\n",
    "- **Attention mechanisms**: How BERT understands context\n",
    "- **Fine-tuning**: Adapting pre-trained models\n",
    "- **Evaluation metrics**: Tracking model performance\n",
    "\n",
    "Great job on completing this comprehensive BERT fine-tuning project! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
