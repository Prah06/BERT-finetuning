# Sentiment Analysis: BERT vs Classical ML Comparative study of transformer models and traditional machine learning for movie review sentiment classification, demonstrating when simpler models outperform complex deep learning. ## Key Finding **Classical ML baseline achieved equivalent performance to BERT (.% vs .%) while being significantly faster and avoiding severe overfitting.** This project demonstrates that complex deep learning models aren't always superior - model selection should be driven by dataset characteristics and task requirements. ## Quick Results | Model | Test Accuracy | Train Accuracy | Overfitting Gap | Training Time | |-------|--------------|----------------|-----------------|---------------| | **TF-IDF + LogReg** | **.%** | .% | .% | < min | | **BERT-base** | .% | .% | .% | m s| *Training time includes Epoch which took significantly longer due to system performance **Key Insight:** BERT's M parameters caused severe overfitting with only k training examples (, params/example ratio). ## Project Structure - **[RESULTS.md](RESULTS.md)** - Detailed experimental results, training curves, and analysis - **[notebooks/sentiment_analysis_enhanced.ipynb](notebooks/)** - Complete implementation with theory explanations - **[models/](models/)** - Saved model artifacts ## Tech Stack - **Deep Learning:** PyTorch, HuggingFace Transformers - **Classical ML:** scikit-learn, TF-IDF - **Training:** CUDA (NVIDIA GPU) - **Dataset:** IMDb k Movie Reviews ## What This Demonstrates ### Technical Skills -Transfer learning with pre-trained transformers -PyTorch training pipelines and GPU acceleration -Overfitting detection and analysis -Model comparison methodology -Classical machine learning fundamentals ### ML Engineering Judgment -When NOT to use deep learning -Cost-benefit analysis (performance vs complexity) -Production-readiness assessment -Data-driven model selection -Understanding dataset size requirements ## Key Insights ### . Severe Overfitting in BERT **Training progression:** - **Epoch :** Train .%, Test .% (Healthy) - **Epoch :** Train .%, Test .% (Slight decline) - **Epoch :** Train .%, Test .% (Best performance) Train accuracy climbed .% while test accuracy stayed essentially flat - classic overfitting pattern. ### . Dataset Size Requirements With , training examples: - **BERT:** , parameters per example (excessive → memorization) - **Baseline:** . parameters per example (appropriate → generalization) **Lesson:** Transformers need k+ examples to justify their M+ parameter complexity. ### . Simplicity Often Wins Despite being ,x larger, BERT provided: - Only .% accuracy gain over baseline - Significantly longer training time (+ minutes vs < minute) - Severe overfitting risk (.% train-test gap) - Complex deployment requirements **Conclusion:** Simple baseline was the optimal choice for this task. ### . Production Considerations | Criterion | TF-IDF | BERT | Winner | |-----------|--------|------|--------| | Accuracy | .% | .% | Tie (~.% diff) | | Training Speed | < min | m s | TF-IDF | | Inference Speed | ~ms | ~ms | TF-IDF | | Overfitting Risk | None | Severe | TF-IDF | | Infrastructure | Simple | Complex (GPU) | TF-IDF | | Interpretability | High | Low | TF-IDF | **Production Recommendation:** Deploy TF-IDF baseline for this use case. ## Detailed Documentation - **[RESULTS.md](RESULTS.md)** - Complete experimental results with epoch-by-epoch analysis - **[notebooks/sentiment_analysis_enhanced.ipynb](notebooks/)** - Implementation with theory explanations covering: - TF-IDF and how it works - BERT architecture and attention mechanisms - Transfer learning concepts - Overfitting detection and analysis - Model selection principles ## When to Use Each Approach ### Use Classical ML (TF-IDF + Logistic Regression) When: -Dataset < k examples -Binary or simple multi-class classification -Speed and simplicity are valued -Model interpretability is required -Limited compute resources ### Use Transformers (BERT) When: -Dataset > k examples -Complex NLP tasks -Context-dependent understanding is critical -Multilingual requirements -You have compute budget and time ## Skills Demonstrated This project showcases understanding of: - **Transformer architecture** - Attention mechanisms, BERT internals, layer-wise representations - **Overfitting causes** - Parameter-to-data ratio, memorization vs generalization - **Model selection** - Matching complexity to task and data availability - **Experimental methodology** - Baseline comparison, ablation analysis - **Production thinking** - Cost-benefit tradeoffs, deployment considerations - **Critical evaluation** - Questioning state-of-the-art when appropriate ## Reproducibility All experiments are fully reproducible using: - Fixed random seeds - Documented hyperparameters - Complete code in notebooks - Detailed results in RESULTS.md ### Environment ``` Python: .+ PyTorch: .+ Transformers: .+ scikit-learn: .+ CUDA: .x ``` See `requirements.txt` for complete dependencies. ## Future Work ### To Improve BERT Performance: . Increase dataset size to k+ examples . Implement early stopping (stop at Epoch ) . Try DistilBERT (M params, less prone to overfitting) . Add stronger regularization (higher dropout) . Implement data augmentation (paraphrasing, back-translation) ### Additional Experiments: - Multi-class sentiment (-star ratings) - Cross-domain evaluation (train on movies, test on products) - Attention visualization to interpret BERT's decisions - Model compression (quantization, pruning) - Compare against other architectures (RoBERTa, ELECTRA) ## 📫 Contact **Prahalad M** | Current Student @ Georgia Institute of Technology [LinkedIn](https://www.linkedin.com/in/prahalad-muralidharan-baa/) | pmuralitharan@gatech.edu **Built as part of portfolio demonstrating modern NLP techniques and ML engineering best practices.** *This project validates the principle: Intelligent solutions / models arent about how complex the solution is rather how fast , simple and effecient one is*