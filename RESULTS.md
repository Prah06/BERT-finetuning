# Experimental Results Detailed results from comparing BERT transformer against classical ML baseline for sentiment analysis. ## Experiment Setup ### Dataset - **Source:** IMDb Movie Reviews - **Total Size:** , reviews - **Training Set:** , reviews (, positive, , negative) - **Test Set:** , reviews (, positive, , negative) - **Task:** Binary sentiment classification - **Class Balance:** Perfect / split (no class imbalance) ### Hardware & Environment - **GPU:** NVIDIA GeForce RTX (CUDA enabled) - **Python:** .. - **PyTorch:** ..+cu - **Transformers:** Latest HuggingFace - **RAM:** GB - **Training Device:** CUDA --- ## Experiment : TF-IDF Baseline ### Configuration ```python Vectorizer: TfidfVectorizer Max Features: , N-grams: (, ) # Unigrams and bigrams Classifier: LogisticRegression Solver: lbfgs Max Iterations: Random State: None ``` ### Results | Metric | Value | |--------|-------| | **Training Accuracy** | ~.% | | **Test Accuracy** | **.%** | | **Train-Test Gap** | .% | | **Training Time** | < minute | | **Parameters** | ~, | ### Classification Report ``` precision recall f-score support Negative . . . , Positive . . . , accuracy . , macro avg . . . , weighted avg . . . , ``` ### Analysis **No overfitting:** Train and test accuracy nearly identical (.% gap) **Balanced performance:** Equal precision/recall for both classes **Fast training:** Complete pipeline in under minute **Interpretable:** Can examine feature weights **Key Observation:** Simple model achieving ~% accuracy with zero overfitting signals this is likely near the performance ceiling for this dataset with bag-of-words approaches. --- ## Experiment : BERT Fine-Tuning ### Configuration ```python Model: bert-base-uncased Parameters: ,, Epochs: Batch Size: Learning Rate: e- Optimizer: AdamW Weight Decay: . Scheduler: Linear decay with warmup Warmup Steps: Max Sequence Length: Device: CUDA (GPU) ``` ### Training Progression #### Epoch ``` Duration: minutes seconds Batches: / Train Loss: . Train Accuracy: .% Test Loss: . Test Accuracy: .% Train-Test Gap: -.% (Good generalization - test performing better) ``` **Analysis:** Initial training showing good generalization. Test accuracy slightly below train indicates model hasn't started overfitting yet. Loss is decreasing appropriately. --- #### Epoch ``` Duration: minutes seconds Batches: / Train Loss: . Train Accuracy: .% Test Loss: . Test Accuracy: .% Train-Test Gap: +.% (Starting to overfit) ``` **Analysis:** Clear overfitting signal: - Train accuracy jumped .% (.% → .%) - Test accuracy improved .% (.% → .%) - Train loss halved (. → .) - Test loss increased (. → .) This shows the model starting to overfit. --- #### Epoch ``` Duration: minutes seconds Batches: / Train Loss: . Train Accuracy: .% Test Loss: . Test Accuracy: .% Train-Test Gap: +.% (Overfitting detected!) ``` *Note: Epoch experienced significant system slowdown, taking much longer than expected **Analysis:** Severe overfitting confirmed: - Train accuracy reached .% (significant memorization) - Test accuracy improved to .% (best performance) - Train loss dropped to . (model fitting training data well) - Test loss increased to . (worse generalization) Best performance achieved at Epoch with .% test accuracy. --- ### Summary Statistics | Metric | Epoch | Epoch | Epoch | Change (→) | |--------|---------|---------|---------|--------------| | **Train Accuracy** | .% | .% | .% | +.% | | **Test Accuracy** | .% | .% | .% | +.% | | **Train Loss** | . | . | . | -.% | | **Test Loss** | . | . | . | +.% | | **Train-Test Gap** | -.% | +.% | +.% | +.% | | **Time per Epoch** | m s | m s | m s | - | ### Overfitting Visualization **Loss Trajectory:** ``` Train Loss: . → . → . ↓↓↓ (Decreasing - fitting training data) Test Loss: . → . → . ↑↑↑ (Increasing - worse generalization) ``` **Accuracy Trajectory:** ``` Train Acc: .% → .% → .% ↑↑↑ (Memorizing) Test Acc: .% → .% → .% ↑ (Slow improvement, peaked at end) ``` **Train-Test Gap:** ``` Epoch : -.% Healthy (test better) Epoch : .% Warning Epoch : .% Severe ``` --- ## Root Cause Analysis ### Why BERT Overfitted **. Parameter-to-Example Ratio** ``` Parameters: ,, Training Examples: , Ratio: , parameters per example ``` This ratio is excessive. The model has enough capacity to memorize every single training example multiple times over. **. Dataset Size Insufficient** - BERT designed for datasets with k-M+ examples - Our k examples represent only .% of recommended minimum - Insufficient data to constrain M parameters to generalizable patterns **. Learning Pattern** - **Epoch :** Model learns general sentiment patterns (words like "great", "terrible") - **Epoch :** Begins memorizing specific phrases and review structures - **Epoch :** Memorizing individual reviews verbatim ### Evidence of Memorization **Training accuracy progression:** - % → Model learned general patterns - % → Memorizing common patterns - % → Memorizing specific examples **Test accuracy stagnation:** - Started at % (learned generalizable patterns) - Stayed at % (no new generalizable learning) - No benefit from epochs - --- ## Final Comparison ### Performance Metrics | Model | Test Acc | Train Acc | Gap | Winner | |-------|----------|-----------|-----|--------| | **TF-IDF Baseline** | .% | .% | .% | Better generalization | | **BERT (Epoch )** | .% | .% | .% | Best accuracy, severe overfitting | | **BERT (Epoch )** | .% | .% | .% | Moderate overfitting | **Accuracy Analysis:** - BERT best improvement over baseline: +.% - Not statistically significant given variance - Within measurement error ### Efficiency Metrics | Model | Training Time | Inference Speed | Parameters | Memory | |-------|--------------|-----------------|------------|--------| | **TF-IDF** | < min | ~ms/sample | K | ~MB | | **BERT** | m s | ~ms/sample | M | ~MB | **Training Time Breakdown:** - Epoch : m s - Epoch : m s - Epoch : m s - Total: m s **Speed Comparison:** - BERT training: x slower than TF-IDF - BERT inference: x slower than TF-IDF ### Cost-Benefit Analysis **BERT Investment:** - + minutes training time - GPU required ($-/hr cloud cost) - Complex deployment (model size, GPU inference) - Overfitting management needed **BERT Return:** - .% accuracy gain (.% → .%) - Severe overfitting (.% train-test gap) - Achieved best performance at final epoch despite overfitting **Verdict:** Cost >> Benefit for this use case --- ## Production Recommendations ### For This Specific Task (Movie Sentiment, k examples): **Deploy:** TF-IDF + Logistic Regression **Rationale:** . Equivalent accuracy (.% vs .%) . Zero overfitting vs severe overfitting . x faster training . x faster inference . Simpler deployment (CPU-only, small model size) . Interpretable (can examine feature weights) . Easy to debug and maintain ### When BERT Would Be Justified: Only if you had: . **x more data** (k+ examples) . **More complex task** (multi-class, aspect-based sentiment) . **Context requirements** (sarcasm detection, nuanced language) . **GPU budget** for both training and inference . **Time for proper regularization** (early stopping, dropout tuning) --- ## Lessons Learned ### . Dataset Size is Critical **Our experience:** - k examples insufficient for M parameter model - Parameters-per-example ratio of ,: led to memorization - Baseline with k parameters (.: ratio) generalized perfectly **Guideline established:** ``` Model Size Required Dataset Our Dataset Verdict K params → k-k examples → k examples Plenty M params → k-M examples → k examples Insufficient ``` ### . Overfitting Patterns **Classic overfitting signature observed:** ``` Train loss decreases (model fitting training data) Test loss increases (model not generalizing) Train accuracy increases (memorization) Test accuracy plateaus (no real learning) Train-test gap widens (divergence) ``` **Early detection crucial:** - Epoch : Gap .% (healthy) - Epoch : Gap .% (should have stopped here) - Epoch : Gap .% (wasted + hours) ### . Baseline Comparison Essential **Without baseline:** - "BERT achieved % accuracy" sounds decent - No context for evaluation **With baseline:** - "BERT achieved % vs .% baseline" - "Cost: x longer training, severe overfitting" - "Gain: .% (not significant)" - Clear decision: baseline wins ### . Production vs Research **Research mindset:** - "How high can we push accuracy?" - "What's the state-of-the-art?" **Production mindset:** - "What's the simplest solution that meets requirements?" - "What's the cost-benefit?" - "How will this scale?" **Our conclusion:** Production mindset chose baseline. --- ## Future Work ### Immediate Next Steps **If required to improve BERT:** . **Early stopping** - Use Epoch results (.%, gap .%) . **Increase dropout** - Default . → .-. . **Try DistilBERT** - M params (% smaller) . **Reduce learning rate** - e- → e- **Expected impact:** Maybe reach -% without severe overfitting ### Longer-Term Improvements **To properly justify transformers:** . **Collect more data** - Target k+ reviews . **Multi-class task** - -star ratings instead of binary . **Cross-domain** - Train on movies, test on products . **Aspect-based sentiment** - Identify sentiment per aspect ### Additional Experiments . **Attention visualization** - Understand what BERT learned . **Feature importance** - Compare BERT vs TF-IDF patterns . **Error analysis** - Where does each model fail? . **Ensemble** - Combine BERT + baseline predictions . **Active learning** - Sample most informative examples --- ## Reproducibility ### Fixed Parameters - Random seeds: Not set (variation expected) - Train/test split: Fixed by IMDb dataset - Hyperparameters: Documented above ### Expected Variance - Accuracy: ±.% across runs - Training time: ±% (system dependent) - Loss values: ±% ### To Reproduce . Clone repository . Install requirements (`requirements.txt`) . Run notebook: `notebooks/sentiment_analysis_enhanced.ipynb` . Results should match within expected variance --- ## Conclusions **This experiment validates fundamental ML principles:** . **Always benchmark against simple baselines** . **Model complexity must scale with dataset size** . **Monitor train-test gap for overfitting** . **Cost-benefit analysis drives production decisions** . **State-of-the-art ≠ Best solution** **For this specific task:** - k movie reviews - Binary sentiment classification - Production deployment **Winner: TF-IDF + Logistic Regression** Simple, fast, accurate, and reliable. **Last Updated:** January **Experiment Conducted By:** Prahalad M | Georgia Tech MS Analytics